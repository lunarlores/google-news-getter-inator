{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f885870-784d-4c99-8721-0e0c7f0eb529",
   "metadata": {},
   "source": [
    "**Before starting, make sure GeckoDriver is installed and on path. Gecko driver install guide is paired with this file. Code BELOW does a check.** \n",
    "Google's robots.txt file disallows scraping for certain pages, including Google Search and Google News. This is strictly for personal use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a794b1-4088-4c5b-a706-8436efec5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_geckodriver():\n",
    "    try:\n",
    "        # Execute the command to get the GeckoDriver version\n",
    "        result = subprocess.run(['geckodriver', '--version'], capture_output=True, text=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(\"GeckoDriver is on PATH.\")\n",
    "            print(\"Version:\", result.stdout.strip())\n",
    "        else:\n",
    "            print(\"GeckoDriver is not on PATH.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"GeckoDriver is not found. Please ensure it is installed and in your PATH.\")\n",
    "\n",
    "# Call the function to check GeckoDriver status\n",
    "check_geckodriver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8d3ab-7cf7-4bdf-860b-694344859b7a",
   "metadata": {},
   "source": [
    "# Setting Up Your Firefox Browser Profile\n",
    "\n",
    "1. First, go to `about:profiles` in your Firefox browser.\n",
    "2. Click **Create a New Profile**.\n",
    "3. Click **Next**.\n",
    "4. Enter a new unique profile name (e.g., `selenium`).\n",
    "5. Scroll down to find your newly created profile.\n",
    "6. Click **Launch Profile in New Browser**.\n",
    "\n",
    "For everyday browsing, you'll want to continue using your **main profile**. The profile you just created is specifically for the Python bot to use. You cannot use this new profile simultaneously while the Python bot is running. However, you can have multiple Firefox profiles open at the same time, just not the same one that's being used by the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06d7c5-fcf4-4de7-8924-806cbd1d977c",
   "metadata": {},
   "source": [
    "# Changing User Agent Extension in Firefox - for your selenium profile\n",
    "\n",
    "1. Go to [User Agent String Switcher](https://addons.mozilla.org/en-US/firefox/addon/user-agent-string-switcher/).\n",
    "2. Click **Add to Firefox**.\n",
    "3. Click the puzzle icon beside the address bar.\n",
    "4. Click on the extension **User-Agent Switcher and Manager**.\n",
    "5. In the drop-down menu, select **Firefox** (it initially shows Chrome), then select **Android** (it initially shows Windows or Mac), and finally, choose the latest user-agent.\n",
    "6. Click **Apply (container)**.\n",
    "\n",
    "If it says \"User-Agent is set,\" the change was successful. You can click **Reset (container)** to revert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db59001-0163-4a3b-99a5-d3c846631e2c",
   "metadata": {},
   "source": [
    "# Turning Off JavaScript in Firefox\n",
    "\n",
    "1. Go to [Disable JavaScript Add-on](https://addons.mozilla.org/en-US/firefox/addon/disable-javascript/).\n",
    "2. Click **Add to Firefox** to install the extension.\n",
    "3. After installation, click the puzzle icon beside the address bar.\n",
    "4. Right-click the **Disable JavaScript** icon and select **Pin to Toolbar** to make it easily accessible.\n",
    "5. Right-click the **Disable JavaScript** icon again, click **Open Disable JS Settings**, then set **Default State** to **JS off**.\n",
    "\n",
    "To enable JavaScript again, adjust the settings or click the icon as needed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6ea6b8a-e8a1-4d61-bb33-dc9d87beeb8b",
   "metadata": {},
   "source": [
    "# Important Settings for Firefox Profile\n",
    "\n",
    "## Profile Information\n",
    "\n",
    "| Setting            | Value                        |\n",
    "|--------------------|------------------------------|\n",
    "| Default Profile     | no                           |\n",
    "| Root Directory      | `C:\\Users\\YourUserName\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\xxxxxxxx.selenium` |\n",
    "| Local Directory      | `C:\\Users\\YourUserName\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\xxxxxxxx.selenium` |\n",
    "\n",
    "*Your profile path is the one in the root directory. xxxxxxxx is a sensitive unique code of your profile, be careful with it and do not make it public*\n",
    "\n",
    "## Disable JavaScript Settings\n",
    "\n",
    "### Settings\n",
    "\n",
    "| Setting                     | Option     |\n",
    "|-----------------------------|------------|\n",
    "| Default state               | JS on / **JS off** |\n",
    "| Disable behavior             | By domain / By tab |\n",
    "| Enable shortcuts            | Yes / No   |\n",
    "| Enable context menu item    | Yes / No   |\n",
    "\n",
    "Set Default state to Js off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273672e2-8d22-4667-a64e-73df2c70b012",
   "metadata": {},
   "source": [
    "# TOOL 1: Article Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c55a4e-cb4d-452b-9fdd-9d48623c28c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Web Scraping with Selenium and Tkinter\n",
    "\n",
    "This Python script performs web scraping on Google News using Selenium and provides a user-friendly GUI for input. The main functionalities include scraping news articles based on a user-defined search term and saving the results to an Excel file. The user can also control the scraping process through the GUI.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "- **Pandas**: For handling and saving data in Excel format.\n",
    "- **Selenium**: For automating web browser interaction.\n",
    "- **Tkinter**: For creating the GUI for user input and status updates.\n",
    "- **Datetime**: For formatting timestamps in the output filenames.\n",
    "- **Threading**: For running the scraping process in a separate thread to keep the GUI responsive.\n",
    "\n",
    "### Function Definitions\n",
    "\n",
    "1. **`human_scroll(driver)`**: \n",
    "   - Simulates human-like scrolling on the webpage to load dynamic content.\n",
    "   - Introduces random pauses between scrolls for a more natural behavior.\n",
    "\n",
    "2. **`maybe_idle()`**: \n",
    "   - Randomly simulates idle time to mimic a user who occasionally pauses.\n",
    "\n",
    "3. **`process_google_search_links(link)`**: \n",
    "   - Processes the links obtained from Google search results to extract the actual URLs from Google redirects.\n",
    "\n",
    "4. **`scrape_google_news(driver, search_url)`**: \n",
    "   - Scrapes the news articles from the provided search URL.\n",
    "   - Extracts titles, links, publishers, dates, and bylines from the search results.\n",
    "   - Handles any exceptions that occur during the scraping process.\n",
    "\n",
    "5. **`run_scraping(driver, encoded_query)`**: \n",
    "   - Manages the overall scraping process.\n",
    "   - Iterates through search results, scraping news articles, and saving progress to an Excel file every 10 pages.\n",
    "   - Checks if scraping should continue based on user input.\n",
    "\n",
    "6. **`get_profile_path()`**: \n",
    "   - Prompts the user to enter the path to their Selenium profile, validating the input.\n",
    "\n",
    "7. **`get_search_term()`**: \n",
    "   - Prompts the user to enter a search term and confirms the choice.\n",
    "\n",
    "8. **`stop_scraping()`**: \n",
    "   - Stops the scraping process when called by the user through the GUI.\n",
    "\n",
    "### GUI Implementation\n",
    "\n",
    "- The script creates a Tkinter GUI that:\n",
    "  - Asks for the Selenium profile path.\n",
    "  - Asks for the search term and confirms the userâ€™s intention to search.\n",
    "  - Displays the scraping progress and provides a button to stop the process.\n",
    "\n",
    "### Scraping Logic\n",
    "\n",
    "1. The user is prompted to enter the path to their Selenium profile.\n",
    "2. The user is then asked for a search term to scrape from Google News.\n",
    "3. The program initiates the scraping process in a separate thread to keep the GUI responsive.\n",
    "4. As the scraping runs, the program processes news articles and saves them to an Excel file.\n",
    "5. The user can stop the scraping process at any time, and the progress will be saved.\n",
    "6. Finally, the program saves the results to a final output file and closes the browser.\n",
    "\n",
    "### File Naming Convention\n",
    "\n",
    "- The output file is named based on the search query and the current timestamp:\n",
    "  - Temporary files are named as `{query}-temp.xlsx`.\n",
    "  - The final results are saved as `{query}-{current_time}-final.xlsx`.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This script provides an efficient way to scrape news articles from Google News while allowing user interaction through a GUI. The use of Selenium ensures that the scraping mimics human behavior, while Tkinter provides an easy-to-use interface for input and status updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b1dc4-7f69-4297-8d98-ce421c42c00a",
   "metadata": {},
   "source": [
    "# Important Notes for Running the Web Scraping Script\n",
    "\n",
    "## Managing Firefox Sessions\n",
    "\n",
    "- **Check Task Manager**: \n",
    "  - Before running the web scraping script, ensure that there are no multiple sessions of Firefox running. Open the Task Manager and look for any instances of Firefox that may be active. \n",
    "  - If there are multiple sessions, consider closing them to prevent conflicts and ensure that the script can run smoothly.\n",
    "\n",
    "## Stopping the Kernel in Jupyter Notebook\n",
    "\n",
    "- **Interrupting the Scraping Process**: \n",
    "  - If you choose to interrupt the scraping process while the program is running, make sure to stop the kernel in Jupyter Notebook.\n",
    "  - This will help free up resources and ensure that no lingering processes are running in the background that could affect future executions of the script.\n",
    "\n",
    "By following these steps, you can ensure a smoother experience when running the web scraping script and avoid potential issues related to multiple browser sessions and active kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ce1f6-c074-48be-813a-1598db9c88bf",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd9086-9d94-4aa5-859c-79814bdbe462",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas selenium openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74fd89-51bd-447a-ab33-f4b7a0e4bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "# Global variable to control the scraping process\n",
    "scraping_active = True\n",
    "\n",
    "# Function to perform human-like scrolling\n",
    "def human_scroll(driver):\n",
    "    scroll_pause = random.uniform(2, 5)  # Random pause between 2 to 5 seconds\n",
    "    scroll_height = random.randint(200, 800)  # Scroll by a random height\n",
    "    driver.execute_script(f\"window.scrollBy(0, {scroll_height});\")  # Scroll the page\n",
    "    time.sleep(scroll_pause)  # Wait for the pause duration\n",
    "\n",
    "# Function to simulate idle time\n",
    "def maybe_idle():\n",
    "    if random.random() < 0.2:  # 20% chance of idle\n",
    "        idle_time = random.uniform(2, 5)  # Idle for 2 to 5 seconds\n",
    "        print(f\"Idling for {idle_time:.2f} seconds...\")\n",
    "        time.sleep(idle_time)  # Pause the script to simulate being idle\n",
    "\n",
    "# Function to process Google search links and extract real article URLs\n",
    "def process_google_search_links(link):\n",
    "    if 'url?q=' in link:\n",
    "        # Extract the real URL from Google redirect\n",
    "        actual_link = link.split('url?q=')[1].split('&')[0]\n",
    "        return actual_link\n",
    "    else:\n",
    "        return link  # It's a direct article link, so return as is\n",
    "\n",
    "# Function to scrape Google News using consistent nth-child selectors\n",
    "def scrape_google_news(driver, search_url):\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    # Simulate human scrolling\n",
    "    human_scroll(driver)\n",
    "\n",
    "    # Maybe idle for some time\n",
    "    maybe_idle()\n",
    "\n",
    "    # Fetch the titles, links, publishers, dates, and bylines using nth-child selectors\n",
    "    articles = []\n",
    "    try:\n",
    "        # Loop through div:nth-child(5) to div:nth-child(14)\n",
    "        for i in range(5, 15):  # Adjust the range based on your needs\n",
    "            if not scraping_active:  # Check if scraping is still active\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Dynamically generate the CSS selector for each nth-child\n",
    "                selector = f\"#main > div:nth-child({i}) > div:nth-child(1) > a:nth-child(1)\"\n",
    "                link_element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                raw_link = link_element.get_attribute('href')\n",
    "\n",
    "                # Process the link to handle both Google redirects and direct article links\n",
    "                processed_link = process_google_search_links(raw_link)\n",
    "\n",
    "                # Extract the title\n",
    "                title_element = link_element.find_element(By.TAG_NAME, 'h3')\n",
    "                title_text = title_element.text\n",
    "\n",
    "                # Assuming publisher, date, and byline exist and have a standard structure\n",
    "                publisher_element = driver.find_element(By.CSS_SELECTOR, f'#main > div:nth-child({i}) div.BNeawe.UPmit.AP7Wnd')\n",
    "                date_element = driver.find_element(By.CSS_SELECTOR, f'#main > div:nth-child({i}) span.r0bn4c.rQMQod')\n",
    "                byline_element = driver.find_element(By.CSS_SELECTOR, f'#main > div:nth-child({i}) div.BNeawe.s3v9rd.AP7Wnd')\n",
    "\n",
    "                publisher = publisher_element.text\n",
    "                date = date_element.text\n",
    "                byline = byline_element.text\n",
    "\n",
    "                # Add the collected info\n",
    "                articles.append((title_text, processed_link, publisher, date, byline))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing div:nth-child({i}): {e}\")\n",
    "                continue \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Function to run the scraping in a separate thread\n",
    "def run_scraping(driver, encoded_query):\n",
    "    global scraping_active\n",
    "    all_articles = []\n",
    "    start_index = 0\n",
    "\n",
    "    while scraping_active:\n",
    "        search_url = f\"https://www.google.com/search?q={encoded_query}&tbm=nws&start={start_index}\"\n",
    "        print(f\"Processing URL: {search_url}\")\n",
    "        \n",
    "        articles = scrape_google_news(driver, search_url)\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"No more articles found. Stopping the scrape.\")\n",
    "            break\n",
    "\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "        # Update status message\n",
    "        status_var.set(f\"Processed {start_index // 10 + 1} pages...\")\n",
    "\n",
    "        # Save progress to an Excel file every 10 pages\n",
    "        if start_index % 10 == 0:\n",
    "            output_file_path = f\"{query}-temp.xlsx\"\n",
    "            articles_df = pd.DataFrame(all_articles, columns=['Title', 'Link', 'Publisher', 'Date', 'Byline'])\n",
    "            articles_df.to_excel(output_file_path, index=False)\n",
    "            status_var.set(f\"Saved progress to {output_file_path}\")\n",
    "\n",
    "        # Increment start_index for the next batch of results\n",
    "        start_index += 10\n",
    "\n",
    "    # Final save after exiting the loop\n",
    "    current_time = datetime.now().strftime(\"%d%m%Y%H%M\")\n",
    "    output_file_path = f\"{query}-{current_time}-final.xlsx\"\n",
    "    articles_df = pd.DataFrame(all_articles, columns=['Title', 'Link', 'Publisher', 'Date', 'Byline'])\n",
    "    articles_df.to_excel(output_file_path, index=False)\n",
    "    status_var.set(f\"Scraping completed! Final results saved to {output_file_path}\")\n",
    "\n",
    "    # Quit driver after saving\n",
    "    driver.quit()\n",
    "\n",
    "# Set up the GUI for profile path and search term\n",
    "def get_profile_path():\n",
    "    while True:\n",
    "        profile_path = simpledialog.askstring(\"Input\", \"Enter the path to your Selenium profile:\")\n",
    "        if profile_path:\n",
    "            try:\n",
    "                # Attempt to use the profile path\n",
    "                return profile_path\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", \"Invalid path. Please try again.\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"Please enter a valid path.\")\n",
    "\n",
    "# Set up the GUI for search term confirmation\n",
    "def get_search_term():\n",
    "    while True:\n",
    "        query = simpledialog.askstring(\"Input\", \"Enter your search query:\", initialvalue=\"\")\n",
    "        if query is not None:\n",
    "            confirm = messagebox.askyesno(\"Confirm\", f\"Are you sure you want to search for '{query}'?\")\n",
    "            if confirm:\n",
    "                return query\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"Please enter a valid search term.\")\n",
    "\n",
    "# Function to stop scraping\n",
    "def stop_scraping():\n",
    "    global scraping_active\n",
    "    scraping_active = False\n",
    "    status_var.set(\"Scraping stopped. Saving progress...\")\n",
    "\n",
    "# Set up Firefox options to use the logged-in profile\n",
    "profile_path = get_profile_path()\n",
    "firefox_options = Options()\n",
    "firefox_options.add_argument(\"-profile\")\n",
    "firefox_options.add_argument(profile_path)\n",
    "\n",
    "# Initialize the WebDriver (Firefox)\n",
    "driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "# Set up the status message variable\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "status_var = tk.StringVar()\n",
    "status_window = tk.Toplevel()\n",
    "status_window.title(\"Scraping Status\")\n",
    "status_label = tk.Label(status_window, textvariable=status_var, wraplength=300)\n",
    "status_label.pack(pady=10)\n",
    "\n",
    "# Create a button to stop scraping\n",
    "stop_button = tk.Button(status_window, text=\"Stop Scraping\", command=stop_scraping)\n",
    "stop_button.pack(pady=5)\n",
    "\n",
    "# Get the search term\n",
    "query = get_search_term()\n",
    "if query:\n",
    "    # Encode the query for the URL\n",
    "    encoded_query = query.replace(\" \", \"+\")\n",
    "    \n",
    "    # Start the scraping in a separate thread\n",
    "    scraping_thread = threading.Thread(target=run_scraping, args=(driver, encoded_query))\n",
    "    scraping_thread.start()\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "status_window.mainloop()\n",
    "\n",
    "# Close the browser once done (moved inside the run_scraping function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba9989-2ddd-4e30-9703-f39873106d88",
   "metadata": {},
   "source": [
    "# TOOL 2: Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e7cdd-be7f-4cb7-b782-02175993e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newspaper3k pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefad7bb-41d5-45af-bd46-151f650fbf6b",
   "metadata": {},
   "source": [
    "Make sure to make a new folder to put the articles produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799986d-07aa-4522-b42e-eb87b23fb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "# Function to extract article content from a URL\n",
    "def extract_article_content(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load an Excel file with a file dialog\n",
    "def load_excel_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Excel File\", filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")])\n",
    "    return file_path\n",
    "\n",
    "# Function to choose directory for saving articles\n",
    "def choose_save_directory():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    directory = filedialog.askdirectory(title=\"Select Directory to Save Articles\")\n",
    "    return directory\n",
    "\n",
    "# Load Excel file with the list of URLs\n",
    "file_path = load_excel_file()  # User selects the file\n",
    "if not file_path:\n",
    "    messagebox.showerror(\"Error\", \"No file selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Choose directory to save articles\n",
    "save_dir = choose_save_directory()  # User selects directory\n",
    "if not save_dir:\n",
    "    messagebox.showerror(\"Error\", \"No directory selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Add a new column to track status of each article extraction\n",
    "df['Status'] = ''\n",
    "\n",
    "# Loop through the URLs, fetch the article content, and save to txt files\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column with URLs is named 'Link'\n",
    "    article_text = extract_article_content(url)\n",
    "    \n",
    "    if article_text:\n",
    "        file_name = os.path.join(save_dir, f\"article_{index + 1}.txt\")\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_text)\n",
    "        df.at[index, 'Status'] = 'Successful'\n",
    "        print(f\"Article {index + 1} saved to {file_name}\")\n",
    "    else:\n",
    "        df.at[index, 'Status'] = 'Failed'\n",
    "\n",
    "# Save the Excel file with the status column\n",
    "base_name = os.path.basename(file_path)  # Get the base name of the file\n",
    "name_without_ext = os.path.splitext(base_name)[0]  # Remove the file extension\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), f\"{name_without_ext}_with_status.xlsx\")\n",
    "df.to_excel(output_file_path, index=False)\n",
    "print(f\"Excel file saved with status column at {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdbab2-6568-4b6a-8919-49e85e38b6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
